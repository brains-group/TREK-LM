# Federated Instruction Tuning on General Dataset
---

seed: 42

dataset:
  path: "./data/{}.json"
  name: "movieKnowledgeGraphDatasetWithSyntheticData"

model:
  name: "Qwen/Qwen3-0.6B"
  quantization: 4 # 8 or 4 if you want to do quantization with BitsAndBytes
  gradient_checkpointing: True
  use_fast_tokenizer: False
  lora:
    peft_lora_r: 16
    peft_lora_alpha: 64
    target_modules: ["q_proj","v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

train:
  num_rounds: ${flower.num_rounds}
  save_every_round: 5
  learning_rate_max: 5e-5
  learning_rate_min: 1e-6
  seq_length: 4096
  padding_side: "left"
  evaluate_split: false
  desirable_prompt_weight: 3.0
  undesirable_prompt_weight: 4.0
  training_arguments:
    output_dir: null # to be set by hydra
    learning_rate: null # to be set by the client
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 1
    logging_steps: 10
    num_train_epochs: 3
    report_to: null
    # save_steps: 1000
    # save_total_limit: 10
    gradient_checkpointing: ${model.gradient_checkpointing}
    lr_scheduler_type: "constant"
    seed: ${seed}


flower:
    num_clients: null # to be set by length of dataset
    num_rounds: 128
    sample_clients: 4
    client_resources:
      num_cpus: 1
      num_gpus: 1
  # dp:
  #   noise_multiplier: 1.0
  #   clipping_threshold: 1.0
