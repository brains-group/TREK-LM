{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b920f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 14505\n",
      "number of unique_entities -> 9809\n",
      "number of unique_entities -> 10348\n",
      "Initialised relations and entities from TransE\n",
      "Graph created\n",
      "length of graph keys is  13781\n",
      "time taken  430.30096650123596\n",
      "length of neighbors dict is  13222\n",
      "Total triples count 310116, training triples 272115, validation_triples 17535, test_triples 20466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/KnowledgeCompletionFedLLM/KBGAT/create_batch.py:63: RuntimeWarning: invalid value encountered in cast\n",
      "  (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
      "/home/spadef/KnowledgeCompletionFedLLM/KBGAT/create_batch.py:65: RuntimeWarning: overflow encountered in cast\n",
      "  (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening node_neighbors pickle object\n",
      "Initial entity dimensions torch.Size([14541, 100]) , relation dimensions torch.Size([237, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from models import SpKBGATModified, SpKBGATConvOnly\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import deepcopy\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Context\n",
    "from flwr.common.typing import NDArrays, Scalar\n",
    "\n",
    "from preprocess import (\n",
    "    read_entity_from_id,\n",
    "    read_relation_from_id,\n",
    "    init_embeddings,\n",
    "    build_data,\n",
    ")\n",
    "from create_batch import Corpus\n",
    "from utils import save_model\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "from typing import Callable, Dict, Tuple, List\n",
    "from collections import OrderedDict\n",
    "from logging import ERROR\n",
    "\n",
    "data = \"FB15k-237\"\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    args = argparse.ArgumentParser()\n",
    "    # network arguments\n",
    "    args.add_argument(\n",
    "        \"-data\",\n",
    "        \"--data\",\n",
    "        default=f\"./data/{data}/\",\n",
    "        help=\"data directory\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-e_g\", \"--epochs_gat\", type=int, default=45, help=\"Number of epochs\"\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-e_c\", \"--epochs_conv\", type=int, default=3, help=\"Number of epochs\"\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-w_gat\",\n",
    "        \"--weight_decay_gat\",\n",
    "        type=float,\n",
    "        default=0.00001,\n",
    "        help=\"L2 reglarization for gat\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-w_conv\",\n",
    "        \"--weight_decay_conv\",\n",
    "        type=float,\n",
    "        default=0.000001,\n",
    "        help=\"L2 reglarization for conv\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-pre_emb\",\n",
    "        \"--pretrained_emb\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Use pretrained embeddings\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-emb_size\",\n",
    "        \"--embedding_size\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Size of embeddings (if pretrained not used)\",\n",
    "    )\n",
    "    args.add_argument(\"-l\", \"--lr\", type=float, default=1e-3)\n",
    "    args.add_argument(\"-g2hop\", \"--get_2hop\", type=bool, default=True)\n",
    "    args.add_argument(\"-u2hop\", \"--use_2hop\", type=bool, default=True)\n",
    "    args.add_argument(\"-p2hop\", \"--partial_2hop\", type=bool, default=True)\n",
    "    args.add_argument(\n",
    "        \"-outfolder\",\n",
    "        \"--output_folder\",\n",
    "        default=f\"../models/KBGAT/{data}/{(datetime.now()).strftime(\"%Y%m%d%H%M%S\")}\",\n",
    "        help=\"Folder name to save the models.\",\n",
    "    )\n",
    "\n",
    "    # arguments for GAT\n",
    "    args.add_argument(\n",
    "        \"-b_gat\", \"--batch_size_gat\", type=int, default=1360, help=\"Batch size for GAT\"\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-neg_s_gat\",\n",
    "        \"--valid_invalid_ratio_gat\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Ratio of valid to invalid triples for GAT training\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-drop_GAT\",\n",
    "        \"--drop_GAT\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Dropout probability for SpGAT layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-alpha\",\n",
    "        \"--alpha\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"LeakyRelu alphs for SpGAT layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-out_dim\",\n",
    "        \"--entity_out_dim\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[100, 200],\n",
    "        help=\"Entity output embedding dimensions\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-h_gat\",\n",
    "        \"--nheads_GAT\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[2, 2],\n",
    "        help=\"Multihead attention SpGAT\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-margin\", \"--margin\", type=float, default=1, help=\"Margin used in hinge loss\"\n",
    "    )\n",
    "\n",
    "    # arguments for convolution network\n",
    "    args.add_argument(\n",
    "        \"-b_conv\",\n",
    "        \"--batch_size_conv\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"Batch size for conv\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-alpha_conv\",\n",
    "        \"--alpha_conv\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"LeakyRelu alphas for conv layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-neg_s_conv\",\n",
    "        \"--valid_invalid_ratio_conv\",\n",
    "        type=int,\n",
    "        default=40,\n",
    "        help=\"Ratio of valid to invalid triples for convolution training\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-o\",\n",
    "        \"--out_channels\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Number of output channels in conv layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"-drop_conv\",\n",
    "        \"--drop_conv\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Dropout probability for convolution layer\",\n",
    "    )\n",
    "\n",
    "    # fed args\n",
    "    args.add_argument(\n",
    "        \"--num_rounds\",\n",
    "        type=float,\n",
    "        default=50,\n",
    "        help=\"Dropout probability for convolution layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--sample_clients\",\n",
    "        type=float,\n",
    "        default=10,\n",
    "        help=\"Dropout probability for convolution layer\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--num_clients\",\n",
    "        type=float,\n",
    "        default=20,\n",
    "        help=\"Dropout probability for convolution layer\",\n",
    "    )\n",
    "\n",
    "    args.add_argument(\"--f\")\n",
    "\n",
    "    args = args.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    (\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        test_data,\n",
    "        entity2id,\n",
    "        relation2id,\n",
    "        headTailSelector,\n",
    "        unique_entities_train,\n",
    "    ) = build_data(args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(\n",
    "            os.path.join(args.data, \"entity2vec.txt\"),\n",
    "            os.path.join(args.data, \"relation2vec.txt\"),\n",
    "        )\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(\n",
    "        args,\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        test_data,\n",
    "        entity2id,\n",
    "        relation2id,\n",
    "        headTailSelector,\n",
    "        args.batch_size_gat,\n",
    "        args.valid_invalid_ratio_gat,\n",
    "        unique_entities_train,\n",
    "        args.get_2hop,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        corpus,\n",
    "        torch.FloatTensor(entity_embeddings),\n",
    "        torch.FloatTensor(relation_embeddings),\n",
    "    )\n",
    "\n",
    "\n",
    "Corpus_, entity_embeddings, relation_embeddings = load_data(args)\n",
    "\n",
    "\n",
    "if args.get_2hop:\n",
    "    file = args.data + \"/2hop.pickle\"\n",
    "    with open(file, \"wb\") as handle:\n",
    "        pickle.dump(\n",
    "            Corpus_.node_neighbors_2hop, handle, protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "\n",
    "if args.use_2hop:\n",
    "    print(\"Opening node_neighbors pickle object\")\n",
    "    file = args.data + \"/2hop.pickle\"\n",
    "    with open(file, \"rb\") as handle:\n",
    "        node_neighbors_2hop = pickle.load(handle)\n",
    "\n",
    "entity_embeddings_copied = deepcopy(entity_embeddings)\n",
    "relation_embeddings_copied = deepcopy(relation_embeddings)\n",
    "\n",
    "print(\n",
    "    \"Initial entity dimensions {} , relation dimensions {}\".format(\n",
    "        entity_embeddings.size(), relation_embeddings.size()\n",
    "    )\n",
    ")\n",
    "# %%\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b348cb40",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m model_conv.cuda()\n\u001b[32m     23\u001b[39m model_conv.eval()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmodel_conv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvKB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCorpus_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCorpus_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mentity2id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kgcfl/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kgcfl/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KnowledgeCompletionFedLLM/KBGAT/layers.py:27\u001b[39m, in \u001b[36mConvKB.forward\u001b[39m\u001b[34m(self, conv_input)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, conv_input):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     batch_size, length, dim = \u001b[43mconv_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# assuming inputs are of the form ->\u001b[39;00m\n\u001b[32m     29\u001b[39m     conv_input = conv_input.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "model_conv = SpKBGATConvOnly(\n",
    "    entity_embeddings,\n",
    "    relation_embeddings,\n",
    "    args.entity_out_dim,\n",
    "    args.entity_out_dim,\n",
    "    args.drop_GAT,\n",
    "    args.drop_conv,\n",
    "    args.alpha,\n",
    "    args.alpha_conv,\n",
    "    args.nheads_GAT,\n",
    "    args.out_channels,\n",
    ")\n",
    "model_conv.load_state_dict(\n",
    "    torch.load(\n",
    "        \"{0}conv/trained_{1}.pth\".format(\n",
    "            \"./checkpoints/fb/out/\", args.epochs_conv - 1\n",
    "        )\n",
    "    ),\n",
    "    strict=False,\n",
    ")\n",
    "\n",
    "model_conv.cuda()\n",
    "model_conv.eval()\n",
    "\n",
    "model_conv.convKB(np.tile(Corpus_.test_indices[0, :], (len(Corpus_.entity2id), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled indices\n",
      "test set length  20466\n",
      "0\n",
      "sample -  1 1\n",
      "1\n",
      "sample -  45 71\n",
      "2\n",
      "sample -  2 1\n",
      "3\n",
      "sample -  26 572\n",
      "4\n",
      "sample -  7 254\n",
      "5\n",
      "sample -  1938 2490\n",
      "6\n",
      "sample -  15 590\n",
      "7\n",
      "sample -  1 47\n",
      "8\n",
      "sample -  384 7\n",
      "9\n",
      "sample -  60 209\n",
      "10\n",
      "sample -  8 2\n",
      "11\n",
      "sample -  1029 518\n",
      "12\n",
      "sample -  29 10\n",
      "13\n",
      "sample -  45 1\n",
      "14\n",
      "sample -  2998 562\n",
      "15\n",
      "sample -  777 28\n",
      "16\n",
      "sample -  21 1\n",
      "17\n",
      "sample -  11 390\n",
      "18\n",
      "sample -  1 41\n",
      "19\n",
      "sample -  1 44\n",
      "20\n",
      "sample -  31 9\n",
      "21\n",
      "sample -  4 2\n",
      "22\n",
      "sample -  25 43\n",
      "23\n",
      "sample -  1 2\n",
      "24\n",
      "sample -  2 3\n",
      "25\n",
      "sample -  100 2\n",
      "26\n",
      "sample -  1078 1414\n",
      "27\n",
      "sample -  182 21\n",
      "28\n",
      "sample -  1997 145\n",
      "29\n",
      "sample -  224 6035\n",
      "30\n",
      "sample -  17 9\n",
      "31\n",
      "sample -  394 8\n",
      "32\n",
      "sample -  2 2\n",
      "33\n",
      "sample -  4 11\n",
      "34\n",
      "sample -  2 36\n",
      "35\n",
      "sample -  243 17\n",
      "36\n",
      "sample -  7 43\n",
      "37\n",
      "sample -  70 1359\n",
      "38\n",
      "sample -  43 47\n",
      "39\n",
      "sample -  48 1\n",
      "40\n",
      "sample -  4 2\n",
      "41\n",
      "sample -  214 122\n",
      "42\n",
      "sample -  14 1\n",
      "43\n",
      "sample -  10 967\n",
      "44\n",
      "sample -  22 650\n",
      "45\n",
      "sample -  24 1\n",
      "46\n",
      "sample -  114 5\n",
      "47\n",
      "sample -  15 2\n",
      "48\n",
      "sample -  385 41\n",
      "49\n",
      "sample -  6 106\n",
      "50\n",
      "sample -  2 12\n",
      "51\n",
      "sample -  4 3\n",
      "52\n",
      "sample -  33 739\n",
      "53\n",
      "sample -  5090 828\n",
      "54\n",
      "sample -  656 1330\n",
      "55\n",
      "sample -  279 77\n",
      "56\n",
      "sample -  1290 25\n",
      "57\n",
      "sample -  2 1\n",
      "58\n",
      "sample -  1 18\n",
      "59\n",
      "sample -  4 11\n",
      "60\n",
      "sample -  16 2\n",
      "61\n",
      "sample -  131 602\n",
      "62\n",
      "sample -  49 12\n",
      "63\n",
      "sample -  231 126\n",
      "64\n",
      "sample -  5 7\n",
      "65\n",
      "sample -  50 107\n",
      "66\n",
      "sample -  549 53\n",
      "67\n",
      "sample -  9 2\n",
      "68\n",
      "sample -  21 32\n",
      "69\n",
      "sample -  277 7\n",
      "70\n",
      "sample -  2 138\n",
      "71\n",
      "sample -  17 3\n",
      "72\n",
      "sample -  4 3\n",
      "73\n",
      "sample -  49 9\n",
      "74\n",
      "sample -  1 4\n",
      "75\n",
      "sample -  2 2\n",
      "76\n",
      "sample -  142 276\n",
      "77\n",
      "sample -  1 39\n",
      "78\n",
      "sample -  61 10\n",
      "79\n",
      "sample -  14 1\n",
      "80\n",
      "sample -  22 49\n",
      "81\n",
      "sample -  89 544\n",
      "82\n",
      "sample -  119 4\n",
      "83\n",
      "sample -  702 14\n",
      "84\n",
      "sample -  228 1082\n",
      "85\n",
      "sample -  2 2\n",
      "86\n",
      "sample -  60 296\n",
      "87\n",
      "sample -  1 44\n",
      "88\n",
      "sample -  2497 16\n",
      "89\n",
      "sample -  57 86\n",
      "90\n",
      "sample -  3 9\n",
      "91\n",
      "sample -  42 16\n",
      "92\n",
      "sample -  1 10\n",
      "93\n",
      "sample -  11 38\n",
      "94\n",
      "sample -  336 50\n",
      "95\n",
      "sample -  47 4\n",
      "96\n",
      "sample -  2 2\n",
      "97\n",
      "sample -  11 5\n",
      "98\n",
      "sample -  2 1\n",
      "99\n",
      "sample -  17 12\n",
      "100\n",
      "sample -  1 1\n",
      "101\n",
      "sample -  2 2\n",
      "102\n",
      "sample -  103 2\n",
      "103\n",
      "sample -  3876 606\n",
      "104\n",
      "sample -  2 2\n",
      "105\n",
      "sample -  1 1\n",
      "106\n",
      "sample -  17 1\n",
      "107\n",
      "sample -  2541 2222\n",
      "108\n",
      "sample -  69 9\n",
      "109\n",
      "sample -  179 26\n",
      "110\n",
      "sample -  273 1\n",
      "111\n",
      "sample -  1 1\n",
      "112\n",
      "sample -  150 24\n",
      "113\n",
      "sample -  2 255\n",
      "114\n",
      "sample -  2 1\n",
      "115\n",
      "sample -  56 2\n",
      "116\n",
      "sample -  1 1\n",
      "117\n",
      "sample -  1 2\n",
      "118\n",
      "sample -  3 1\n",
      "119\n",
      "sample -  686 22\n",
      "120\n",
      "sample -  1 1\n",
      "121\n",
      "sample -  1 1\n",
      "122\n",
      "sample -  7 103\n",
      "123\n",
      "sample -  3 34\n",
      "124\n",
      "sample -  3 2\n",
      "125\n",
      "sample -  1474 1120\n",
      "126\n",
      "sample -  193 12\n",
      "127\n",
      "sample -  1 1\n",
      "128\n",
      "sample -  1 1\n",
      "129\n",
      "sample -  5 39\n",
      "130\n",
      "sample -  38 140\n",
      "131\n",
      "sample -  1055 35\n",
      "132\n",
      "sample -  1 2\n",
      "133\n",
      "sample -  582 12\n",
      "134\n",
      "sample -  20 22\n",
      "135\n",
      "sample -  9 3\n",
      "136\n",
      "sample -  3 4\n",
      "137\n",
      "sample -  5 84\n",
      "138\n",
      "sample -  398 153\n",
      "139\n",
      "sample -  2 1\n",
      "140\n",
      "sample -  3 4\n",
      "141\n",
      "sample -  3 1\n",
      "142\n",
      "sample -  316 5\n",
      "143\n",
      "sample -  29 3\n",
      "144\n",
      "sample -  7 13\n",
      "145\n",
      "sample -  2 73\n",
      "146\n",
      "sample -  153 28\n",
      "147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     11\u001b[39m         Corpus_.get_validation_pred(args, model_conv, unique_entities)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mevaluate_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCorpus_\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_entities_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mevaluate_conv\u001b[39m\u001b[34m(args, unique_entities)\u001b[39m\n\u001b[32m      9\u001b[39m model_conv.eval()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mCorpus_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validation_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_entities\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KnowledgeCompletionFedLLM/KBGAT/create_batch.py:497\u001b[39m, in \u001b[36mCorpus.get_validation_pred\u001b[39m\u001b[34m(self, args, model, unique_entities)\u001b[39m\n\u001b[32m    492\u001b[39m sorted_scores_tail, sorted_indices_tail = torch.sort(\n\u001b[32m    493\u001b[39m     scores_tail.view(-\u001b[32m1\u001b[39m), dim=-\u001b[32m1\u001b[39m, descending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    495\u001b[39m \u001b[38;5;66;03m# Just search for zeroth index in the sorted scores, we appended valid triple at top\u001b[39;00m\n\u001b[32m    496\u001b[39m ranks_tail.append(\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     np.where(\u001b[43msorted_indices_tail\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy() == \u001b[32m0\u001b[39m)[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] + \u001b[32m1\u001b[39m)\n\u001b[32m    498\u001b[39m reciprocal_ranks_tail.append(\u001b[32m1.0\u001b[39m / ranks_tail[-\u001b[32m1\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msample - \u001b[39m\u001b[33m\"\u001b[39m, ranks_head[-\u001b[32m1\u001b[39m], ranks_tail[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def evaluate_conv(args, unique_entities):\n",
    "    model_conv = SpKBGATConvOnly(\n",
    "        entity_embeddings,\n",
    "        relation_embeddings,\n",
    "        args.entity_out_dim,\n",
    "        args.entity_out_dim,\n",
    "        args.drop_GAT,\n",
    "        args.drop_conv,\n",
    "        args.alpha,\n",
    "        args.alpha_conv,\n",
    "        args.nheads_GAT,\n",
    "        args.out_channels,\n",
    "    )\n",
    "    model_conv.load_state_dict(\n",
    "        torch.load(\n",
    "            \"{0}conv/trained_{1}.pth\".format(\n",
    "                \"./checkpoints/fb/out/\", args.epochs_conv - 1\n",
    "            )\n",
    "        ),\n",
    "        strict=False,\n",
    "    )\n",
    "\n",
    "    model_conv.cuda()\n",
    "    model_conv.eval()\n",
    "\n",
    "    model_conv.test(Corpus_.test_indices[0])\n",
    "    # with torch.no_grad():\n",
    "    #     Corpus_.get_validation_pred(args, model_conv, unique_entities)\n",
    "\n",
    "\n",
    "evaluate_conv(args, Corpus_.unique_entities_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f39e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgcfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
