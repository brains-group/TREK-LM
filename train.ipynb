{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import flwr as fl\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from datasets import load_dataset\n",
    "from flwr.client.mod import fixedclipping_mod\n",
    "from flwr.server.strategy import (\n",
    "    DifferentialPrivacyClientSideFixedClipping\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils.utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a859e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  path: ./data/{}.json\n",
      "  name: movieKnowledgeGraphDatasetWithSyntheticData\n",
      "model:\n",
      "  name: Qwen/Qwen3-4B\n",
      "  quantization: 4\n",
      "  gradient_checkpointing: true\n",
      "  use_fast_tokenizer: false\n",
      "  lora:\n",
      "    peft_lora_r: 16\n",
      "    peft_lora_alpha: 64\n",
      "    target_modules:\n",
      "    - q_proj\n",
      "    - v_proj\n",
      "train:\n",
      "  num_rounds: ${flower.num_rounds}\n",
      "  save_every_round: 5\n",
      "  learning_rate_max: 5.0e-05\n",
      "  learning_rate_min: 1.0e-06\n",
      "  seq_length: 2048\n",
      "  padding_side: left\n",
      "  evaluate_split: true\n",
      "  training_arguments:\n",
      "    output_dir: null\n",
      "    learning_rate: null\n",
      "    per_device_train_batch_size: 16\n",
      "    gradient_accumulation_steps: 1\n",
      "    logging_steps: 10\n",
      "    num_train_epochs: 3\n",
      "    max_steps: 10\n",
      "    report_to: null\n",
      "    save_steps: 1000\n",
      "    save_total_limit: 10\n",
      "    gradient_checkpointing: ${model.gradient_checkpointing}\n",
      "    lr_scheduler_type: constant\n",
      "client_resources:\n",
      "  num_cpus: 8\n",
      "  num_gpus: 1.0\n",
      "dp:\n",
      "  noise_mult: 0.02\n",
      "  clip_norm: 0.5\n",
      "flower:\n",
      "  num_clients: null\n",
      "  num_rounds: 1\n",
      "  fraction_fit: 0.02\n",
      "  client_resources:\n",
      "    num_cpus: 8\n",
      "    num_gpus: 1.0\n",
      "  dp:\n",
      "    noise_mult: 0.02\n",
      "    clip_norm: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = get_config(\"federated_full\")\n",
    "\n",
    "print_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5849ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "with open(cfg.dataset.path.format(cfg.dataset.name), \"r\") as file:\n",
    "    datasets = json.load(file)\n",
    "cfg.flower.num_clients = len(datasets.keys())\n",
    "print(cfg.flower.num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5be3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id: 151643\n"
     ]
    }
   ],
   "source": [
    "# ===== Define the tokenizer =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model.name,\n",
    "    use_fast=cfg.model.use_fast_tokenizer,\n",
    "    padding_side=cfg.train.padding_side,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = (\n",
    "        tokenizer.bos_token if cfg.train.padding_side == \"left\" else tokenizer.eos_token\n",
    "    )\n",
    "print(f\"pad_token_id: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921961c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./models/{cfg.model.name}/{cfg.dataset.name}\"\n",
    "client = fl.client.ClientApp(\n",
    "    client_fn=gen_client_fn(\n",
    "        datasets,\n",
    "        tokenizer,\n",
    "        cfg.model,\n",
    "        cfg.train,\n",
    "        save_path,\n",
    "    ),\n",
    "    # mods=[fixedclipping_mod] # For Differential Privacy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context):\n",
    "\n",
    "    # Define the Strategy\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        min_available_clients=cfg.flower.num_clients, # total clients\n",
    "        fraction_fit=cfg.flower.sample_clients/cfg.flower.num_clients, # ratio of clients to sample\n",
    "        fraction_evaluate=0.0, # No federated evaluation\n",
    "        # A (optional) function used to configure a \"fit()\" round\n",
    "        on_fit_config_fn=get_on_fit_config(),\n",
    "        # A (optional) function to aggregate metrics sent by clients\n",
    "        fit_metrics_aggregation_fn=fit_weighted_average,\n",
    "        # A (optional) function to execute on the server after each round. \n",
    "        # In this example the function only saves the global model.\n",
    "        evaluate_fn=get_evaluate_fn( \n",
    "            cfg.model,\n",
    "            cfg.train.save_every_round,\n",
    "            cfg.flower.num_rounds,\n",
    "            save_path\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # # Add Differential Privacy\n",
    "    # sampled_clients = cfg.flower.num_clients*strategy.fraction_fit\n",
    "    # strategy = DifferentialPrivacyClientSideFixedClipping(\n",
    "    #     strategy, \n",
    "    #     noise_multiplier=cfg.flower.dp.noise_mult,\n",
    "    #     clipping_norm=cfg.flower.dp.clip_norm, \n",
    "    #     num_sampled_clients=sampled_clients\n",
    "    # )\n",
    "\n",
    "    # Number of rounds to run the simulation\n",
    "    num_rounds = cfg.flower.num_rounds\n",
    "    config = fl.server.ServerConfig(num_rounds=num_rounds)\n",
    "    \n",
    "    return fl.server.ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "server = fl.server.ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Starting Flower ServerApp, config: num_rounds=1, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [INIT]\n",
      "\u001b[92mINFO \u001b[0m: Requesting initial parameters from one random client\n"
     ]
    }
   ],
   "source": [
    "client_resources = dict(cfg.flower.client_resources)\n",
    "fl.simulation.run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=cfg.flower.num_clients,\n",
    "    backend_config={\"client_resources\": client_resources,\n",
    "                    \"init_args\": backend_setup}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgcfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
